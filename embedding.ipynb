{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 注意(attention)！开始做前必读项！\n",
    "所有的代码一定要在这个文件里面编写，不要自己创建一个新的文件\n",
    "对于提供的数据集，不要改存储地方，也不要修改文件名和内容\n",
    "不要重新定义函数（如果我们已经定义好的），按照里面的思路来编写。当然，除了我们定义的部分，如有需要可以自行定义函数或者模块\n",
    "写完之后，重新看一下哪一部分比较慢，然后试图去优化。一个好的习惯是每写一部分就思考这部分代码的时间复杂度和空间复杂度，AI工程是的日常习惯！\n",
    "这次作业很重要，一定要完成！ 相信会有很多的收获！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/usr/local/anaconda2/envs/pt-tf-env/lib/python3.6/site-packages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim import models\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.externals import joblib\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载训练集文件和测试集文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_text,data=[],[]\n",
    "stopWords=[]\n",
    "\n",
    "def load_data():\n",
    "    '''\n",
    "    函数说明：该函数用于加载数据集\n",
    "    return: \n",
    "        -data: 表示所有数据拼接的原始数据\n",
    "        -data_text: 表示数据集中的特征数据集\n",
    "        -datatext: 表示经过分词之后的特征数据集\n",
    "        -stopWords: 表示读取的停用词\n",
    "    '''\n",
    "    print('load Pre_process')\n",
    "    data = pd.concat([\n",
    "        pd.read_csv('data/train_clean.csv', sep='\\t'),\n",
    "        pd.read_csv('data/dev_clean.csv', sep='\\t'),\n",
    "        pd.read_csv('data/test_clean.csv', sep='\\t')\n",
    "        ])\n",
    "    print(\"读取数据集完成\")\n",
    "    data_text = list(data.text)  # .apply(lambda x: x.split(' '))\n",
    "    datatext = []\n",
    "    for i in range(len(data_text)):\n",
    "        datatext.append(data_text[i].split(' '))\n",
    "    stopWords = open('data/stopwords.txt').readlines()\n",
    "    print(\"取停用词完成\")\n",
    "    return data, data_text, datatext, stopWords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load Pre_process\n",
      "读取数据集完成\n",
      "取停用词完成\n"
     ]
    }
   ],
   "source": [
    "data, data_text, datatext, stopWords = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>category_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>杏林 芳菲 广东 中医药 曹磊 编著 的 杏林 芳菲 广东 中医药 注重 文化 内涵 的 挖...</td>\n",
       "      <td>文化</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>额吉 的 白云 优秀 蒙古文 文学作品 翻译 出版 工程 第五辑 散文 卷 散文集 额吉 的...</td>\n",
       "      <td>文学</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>叶赛宁 抒情诗 选 本书 收入 叶赛宁 的 206 首 抒情诗 , 多为 名篇 佳作 叶赛宁...</td>\n",
       "      <td>文学</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text label  category_id\n",
       "0  杏林 芳菲 广东 中医药 曹磊 编著 的 杏林 芳菲 广东 中医药 注重 文化 内涵 的 挖...    文化            6\n",
       "1  额吉 的 白云 优秀 蒙古文 文学作品 翻译 出版 工程 第五辑 散文 卷 散文集 额吉 的...    文学            0\n",
       "2  叶赛宁 抒情诗 选 本书 收入 叶赛宁 的 206 首 抒情诗 , 多为 名篇 佳作 叶赛宁...    文学            0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['杏林 芳菲 广东 中医药 曹磊 编著 的 杏林 芳菲 广东 中医药 注重 文化 内涵 的 挖掘 以及 特殊 技艺 的 介绍 , 对于 非 物质 文化遗产 广东 中医药 的 内涵 、 技艺 、 形态 、 历史 演变 、 艺术 价值 等 给予 全面 介绍 深刻 而 直观 地 记录 时代 的 变迁 , 记录 民间 丰富 的 生活 , 图文并茂 , 生动活泼 , 富有 艺术 表现力 , 给 读者 以 文化 审美 的 感受',\n",
       " '额吉 的 白云 优秀 蒙古文 文学作品 翻译 出版 工程 第五辑 散文 卷 散文集 额吉 的 白云 共 收入 16 篇 蒙译汉 散文 作品 , 代表 了 建国以来 内蒙古 老中青 三代 蒙古文 散文 创作 的 整体实力 与 成就 这些 散文 作品 抒发 了 作者 对 故乡 故土 、 父辈 亲人 的 思念 热爱 之情 , 也 记录 了 近 三十年 内蒙古 社会 、 文化 、 经济 生活 发生 的 翻天覆地 的 变化 草原 民族 特有 的 浪漫情怀 与 对 草原 生态 现状 的 深切 忧患 跃然纸上 ...',\n",
       " '叶赛宁 抒情诗 选 本书 收入 叶赛宁 的 206 首 抒情诗 , 多为 名篇 佳作 叶赛宁 的 诗 从 上 世纪 80 年代 起 在 我国 得到 广泛 的 传播 , 他 的 诗 饱含 醇厚 的 俄罗斯 民族 的 文化底蕴 , 散发 着 俄罗斯 田野 泥土 的 芳香 及 炽烈 的 诗人 情怀 , 洋溢着 意象 艺术 创新 所 带来 的 奇特 艺术 魅力 当代 著名诗人 多里 佐 对 叶赛宁 及其 诗作 给予 了 这样 的 评价 : 他 属于 那些 也许 几百年 才 产生 几个 的 诗人 , 他 不但 进入 了 俄罗斯 文学 , 而且 已经 进入 俄罗斯 的 风景 , 成为 它 不可分割 的 一部分 ...']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_text[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['杏林',\n",
       "  '芳菲',\n",
       "  '广东',\n",
       "  '中医药',\n",
       "  '曹磊',\n",
       "  '编著',\n",
       "  '的',\n",
       "  '杏林',\n",
       "  '芳菲',\n",
       "  '广东',\n",
       "  '中医药',\n",
       "  '注重',\n",
       "  '文化',\n",
       "  '内涵',\n",
       "  '的',\n",
       "  '挖掘',\n",
       "  '以及',\n",
       "  '特殊',\n",
       "  '技艺',\n",
       "  '的',\n",
       "  '介绍',\n",
       "  ',',\n",
       "  '对于',\n",
       "  '非',\n",
       "  '物质',\n",
       "  '文化遗产',\n",
       "  '广东',\n",
       "  '中医药',\n",
       "  '的',\n",
       "  '内涵',\n",
       "  '、',\n",
       "  '技艺',\n",
       "  '、',\n",
       "  '形态',\n",
       "  '、',\n",
       "  '历史',\n",
       "  '演变',\n",
       "  '、',\n",
       "  '艺术',\n",
       "  '价值',\n",
       "  '等',\n",
       "  '给予',\n",
       "  '全面',\n",
       "  '介绍',\n",
       "  '深刻',\n",
       "  '而',\n",
       "  '直观',\n",
       "  '地',\n",
       "  '记录',\n",
       "  '时代',\n",
       "  '的',\n",
       "  '变迁',\n",
       "  ',',\n",
       "  '记录',\n",
       "  '民间',\n",
       "  '丰富',\n",
       "  '的',\n",
       "  '生活',\n",
       "  ',',\n",
       "  '图文并茂',\n",
       "  ',',\n",
       "  '生动活泼',\n",
       "  ',',\n",
       "  '富有',\n",
       "  '艺术',\n",
       "  '表现力',\n",
       "  ',',\n",
       "  '给',\n",
       "  '读者',\n",
       "  '以',\n",
       "  '文化',\n",
       "  '审美',\n",
       "  '的',\n",
       "  '感受'],\n",
       " ['额吉',\n",
       "  '的',\n",
       "  '白云',\n",
       "  '优秀',\n",
       "  '蒙古文',\n",
       "  '文学作品',\n",
       "  '翻译',\n",
       "  '出版',\n",
       "  '工程',\n",
       "  '第五辑',\n",
       "  '散文',\n",
       "  '卷',\n",
       "  '散文集',\n",
       "  '额吉',\n",
       "  '的',\n",
       "  '白云',\n",
       "  '共',\n",
       "  '收入',\n",
       "  '16',\n",
       "  '篇',\n",
       "  '蒙译汉',\n",
       "  '散文',\n",
       "  '作品',\n",
       "  ',',\n",
       "  '代表',\n",
       "  '了',\n",
       "  '建国以来',\n",
       "  '内蒙古',\n",
       "  '老中青',\n",
       "  '三代',\n",
       "  '蒙古文',\n",
       "  '散文',\n",
       "  '创作',\n",
       "  '的',\n",
       "  '整体实力',\n",
       "  '与',\n",
       "  '成就',\n",
       "  '这些',\n",
       "  '散文',\n",
       "  '作品',\n",
       "  '抒发',\n",
       "  '了',\n",
       "  '作者',\n",
       "  '对',\n",
       "  '故乡',\n",
       "  '故土',\n",
       "  '、',\n",
       "  '父辈',\n",
       "  '亲人',\n",
       "  '的',\n",
       "  '思念',\n",
       "  '热爱',\n",
       "  '之情',\n",
       "  ',',\n",
       "  '也',\n",
       "  '记录',\n",
       "  '了',\n",
       "  '近',\n",
       "  '三十年',\n",
       "  '内蒙古',\n",
       "  '社会',\n",
       "  '、',\n",
       "  '文化',\n",
       "  '、',\n",
       "  '经济',\n",
       "  '生活',\n",
       "  '发生',\n",
       "  '的',\n",
       "  '翻天覆地',\n",
       "  '的',\n",
       "  '变化',\n",
       "  '草原',\n",
       "  '民族',\n",
       "  '特有',\n",
       "  '的',\n",
       "  '浪漫情怀',\n",
       "  '与',\n",
       "  '对',\n",
       "  '草原',\n",
       "  '生态',\n",
       "  '现状',\n",
       "  '的',\n",
       "  '深切',\n",
       "  '忧患',\n",
       "  '跃然纸上',\n",
       "  '...']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datatext[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer_tfidf():\n",
    "    '''\n",
    "    函数说明：该函数用于训练tfidf的词向量\n",
    "    return: \n",
    "        -tfidf: 表示经过TF-ID模型训练出的词向量\n",
    "    '''\n",
    "    print('train tfidf_embedding')\n",
    "    # 训练tfidf的词向量\n",
    "    # Convert a collection of raw documents to a matrix of TF-IDF features.\n",
    "    count_vect = TfidfVectorizer(\n",
    "        stop_words = stopWords, max_df=0.6, ngram_range=(1, 2))\n",
    "    tfidf = count_vect.fit(data.text)\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train tfidf_embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda2/envs/lyy-pt15-env/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:386: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['exp', 'lex', 'sub', 'sup', '①①', '①②', '①③', '①④', '①⑤', '①⑥', '①⑦', '①⑧', '①⑨', '①ａ', '①ｂ', '①ｃ', '①ｄ', '①ｅ', '①ｆ', '①ｇ', '①ｈ', '①ｉ', '①ｏ', '②①', '②②', '②③', '②④', '②⑤', '②⑥', '②⑦', '②⑧', '②⑩', '②ａ', '②ｂ', '②ｃ', '②ｄ', '②ｅ', '②ｆ', '②ｇ', '②ｈ', '②ｉ', '②ｊ', '③①', '③⑩', '③ａ', '③ｂ', '③ｃ', '③ｄ', '③ｅ', '③ｆ', '③ｇ', '③ｈ', '④ａ', '④ｂ', '④ｃ', '④ｄ', '④ｅ', '⑤ａ', '⑤ｂ', '⑤ｄ', '⑤ｅ', '⑤ｆ', '一一', '一个', '一些', '一切', '一则', '一方面', '一旦', '一来', '一样', '一番', '一直', '一般', '万一', '上下', '不仅', '不但', '不光', '不单', '不只', '不如', '不怕', '不惟', '不成', '不拘', '不比', '不然', '不特', '不独', '不管', '不论', '不过', '不问', '与其', '与否', '与此同时', '两者', '为了', '为什么', '为何', '为着', '乃至', '之一', '之所以', '之类', '乌乎', '也好', '也就是说', '也罢', '于是', '于是乎', '云云', '人家', '什么', '什么样', '从而', '他人', '他们', '以便', '以免', '以及', '以至', '以至于', '以致', '任何', '任凭', '似的', '但是', '何况', '何处', '何时', '作为', '你们', '使得', '例如', '依照', '俺们', '倘使', '倘或', '倘然', '倘若', '假使', '假如', '假若', '关于', '其一', '其中', '其二', '其他', '其余', '其它', '其次', '具体地说', '具体说来', '再者', '再说', '况且', '几时', '凭借', '别的', '别说', '前后', '前者', '加之', '即令', '即使', '即便', '即或', '即若', '及其', '及至', '反之', '反过来', '反过来说', '另一方面', '另外', '只是', '只有', '只要', '只限', '叮咚', '可以', '可是', '可见', '各个', '各位', '各种', '各自', '同时', '向着', '否则', '吧哒', '呜呼', '呼哧', '咱们', '哈哈', '哎呀', '哎哟', '哪个', '哪些', '哪儿', '哪天', '哪年', '哪怕', '哪样', '哪边', '哪里', '哼唷', '啪达', '喔唷', '嗡嗡', '嘎登', '因为', '因此', '因而', '固然', '在下', '多少', '她们', '如上所述', '如何', '如其', '如果', '如此', '如若', '宁可', '宁愿', '宁肯', '它们', '对于', '尔后', '尚且', '就是', '就是说', '尽管', '岂但', '并且', '开外', '开始', '当着', '彼此', '怎么', '怎么办', '怎么样', '怎样', '总之', '总的来看', '总的来说', '总的说来', '总而言之', '恰恰相反', '慢说', '我们', '或是', '或者', '所以', '抑或', '按照', '换句话说', '换言之', '接着', '故此', '旁人', '无宁', '无论', '既是', '既然', '时候', '是的', '有些', '有关', '有的', '有的是', '朝着', '本着', '来着', '极了', '果然', '果真', '某个', '某些', '根据', '正如', '此外', '此间', '毋宁', '每当', '比如', '比方', '沿着', '漫说', '然则', '然后', '然而', '照着', '甚么', '甚而', '甚至', '由于', '由此可见', '的话', '相对而言', '省得', '着呢', '第二', '等等', '紧接着', '纵令', '纵使', '纵然', '经过', '结果', '继而', '综上所述', '罢了', '而且', '而况', '而外', '而已', '而是', '而言', '自个儿', '自从', '自各儿', '自家', '自己', '自身', '至于', '若是', '若非', '莫若', '虽则', '虽然', '虽说', '要不', '要不是', '要不然', '要么', '要是', '许多', '设使', '设若', '诸位', '谁知', '起见', '趁着', '越是', '较之', '还是', '还有', '这个', '这么', '这么些', '这么样', '这么点儿', '这些', '这会儿', '这儿', '这就是说', '这时', '这样', '这边', '这里', '进而', '连同', '通过', '遵照', '那个', '那么', '那么些', '那么样', '那些', '那会儿', '那儿', '那时', '那样', '那边', '那里', '鄙人', '鉴于', '除了', '除此之外', '除非', '随着', '非但', '非徒', '顺着', '首先', '１２', 'ｌｉ', 'ｎｇ昉', 'ｚｘｆｉｔｌ'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "tfidf = trainer_tfidf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 根据数据集训练word2vec的词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer_w2v():\n",
    "    '''\n",
    "    函数说明：该函数基于Word2vec模型训练词向量\n",
    "    return: \n",
    "        -w2v: 表示经过word2vec模型训练出的词向量\n",
    "    '''\n",
    "    print('train word2vec Embedding')\n",
    "        # 训练w2v的词向量\n",
    "    w2v = models.Word2Vec(min_count=2,\n",
    "                                window=3,\n",
    "                                size=300,\n",
    "                                sample=6e-5,\n",
    "                                alpha=0.03,\n",
    "                                min_alpha=0.0007,\n",
    "                                negative=15,\n",
    "                                workers=4,\n",
    "                                iter=10,\n",
    "                                max_vocab_size=50000)\n",
    "        \n",
    "    w2v.build_vocab(datatext)\n",
    "\n",
    "    w2v.train(datatext,\n",
    "                       total_examples=w2v.corpus_count,\n",
    "                       epochs=15,\n",
    "                       report_delay=1)\n",
    "        \n",
    "    print('train fast_embedding')\n",
    "    return w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train word2vec Embedding\n"
     ]
    }
   ],
   "source": [
    "w2v = trainer_w2v()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer_fasttext():\n",
    "    '''\n",
    "    函数说明：该函数基于FastText模型训练词向量\n",
    "    return: \n",
    "        -fast: 表示经过FastText模型训练出的词向量\n",
    "    '''\n",
    "    fast = models.FastText(datatext,\n",
    "                                size=300,  # 向量维度\n",
    "                                window=2,  # 移动窗口\n",
    "                                alpha=0.03,\n",
    "                                min_count=2,  # 对字典进行截断，小于该数的则会被切掉,增大该值可以减少词表个数\n",
    "                                iter=10,  # 迭代次数\n",
    "                                min_n=1,\n",
    "                                max_n=3,\n",
    "                                word_ngrams=2,\n",
    "                                max_vocab_size=50000)\n",
    "    return fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast = trainer_fasttext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存生成的词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saver():\n",
    "    '''\n",
    "    函数说明：该函数存储训练好的模型\n",
    "    '''\n",
    "    print('save tfidf model')\n",
    "    joblib.dump(tfidf, 'tfidf_model')\n",
    "    print('save word2vec model')\n",
    "    w2v.save('w2v_model')\n",
    "    print('save fast model')\n",
    "    fast.save('fast_model')\n",
    "    \n",
    "saver()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载模型 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load():\n",
    "    '''\n",
    "    函数说明：该函数加载训练好的模型\n",
    "    '''\n",
    "    print('load tfidf_embedding model')\n",
    "    tfidf = joblib.load('tfidf_model')\n",
    "    print('load w2v_embedding model')\n",
    "    w2v = models.KeyedVectors.load('w2v_model')\n",
    "    print('load fast_embedding model')\n",
    "    fast = models.FastText.load('fast_model')\n",
    "    \n",
    "load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
