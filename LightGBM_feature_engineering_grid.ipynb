{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 图书分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 导入相关的包 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/usr/local/anaconda2/envs/pt-tf-env/lib/python3.6/site-packages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import preprocessing\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import metrics\n",
    "import joblib\n",
    "# from bayes_opt import BayesianOptimization\n",
    "from gensim import models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1: 读取训练好的词向量和训练/测试数据集\n",
    "#### 在运行此文件之前，需要先运行embeddin文件生成词向量文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fast_embedding输出词表的个数19592,w2v_embedding输出词表的个数19592\n",
      "取词向量成功\n",
      "读取数据完成\n"
     ]
    }
   ],
   "source": [
    "max_length = 500  # 表示样本表示最大的长度,表示降维之后的维度\n",
    "sentence_max_length = 1500  # 表示句子/样本在降维之前的维度\n",
    "Train_features3, Test_features3, Train_label3, Test_label3 = [], [], [], []\n",
    "# 加载预训练好的embedding\n",
    "fast_embedding = models.KeyedVectors.load('fast_model')\n",
    "w2v_embedding = models.KeyedVectors.load('w2v_model')\n",
    "\n",
    "print(\"fast_embedding输出词表的个数{},w2v_embedding输出词表的个数{}\".format(\n",
    "    len(fast_embedding.wv.vocab.keys()), len(w2v_embedding.wv.vocab.keys())))\n",
    "\n",
    "print(\"取词向量成功\")\n",
    "\n",
    "train = pd.read_csv('data/train_clean.csv', sep='\\t')\n",
    "#dev = pd.read_csv('dev_clean.tsv', sep='\\t')\n",
    "test = pd.read_csv('data/test_clean.csv', sep='\\t')\n",
    "print(\"读取数据完成\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 将df中的label映射为数字标签并保存到labelIndex列中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelName = train.label.unique()  # 全部label列表\n",
    "labelIndex = list(range(len(labelName)))  # 全部label标签\n",
    "labelNameToIndex = dict(zip(labelName, labelIndex))  # label的名字对应标签的字典\n",
    "labelIndexToName = dict(zip(labelIndex, labelName))  # label的标签对应名字的字典\n",
    "train[\"labelIndex\"] = train.label.map(labelNameToIndex)\n",
    "test[\"labelIndex\"] = test.label.map(labelNameToIndex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         1\n",
      "1         1\n",
      "2         1\n",
      "3         6\n",
      "4         1\n",
      "         ..\n",
      "29469    11\n",
      "29470     1\n",
      "29471    21\n",
      "29472     1\n",
      "29473     5\n",
      "Name: labelIndex, Length: 29474, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(test[\"labelIndex\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "切分数据完成\n"
     ]
    }
   ],
   "source": [
    "def query_cut(query):\n",
    "    '''\n",
    "    该函数用于对输入的语句（query）按照空格进行切分\n",
    "    '''\n",
    "    return query.split(' ')\n",
    "\n",
    "\n",
    "train[\"queryCut\"] = train[\"text\"].apply(query_cut)\n",
    "# dev[\"queryCut\"] = dev[\"text\"].apply(query_cut)\n",
    "test[\"queryCut\"] = test[\"text\"].apply(query_cut)\n",
    "print(\"切分数据完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 读取停用词文件 并去除样本中的停用词\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "去除停用词\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "with open('data/stopwords.txt', \"r\") as f:\n",
    "    stopWords = f.read().split(\"\\n\")\n",
    "def rm_stop_word(wordList):\n",
    "    return [word for word in wordList if word not in stopWords]\n",
    "\n",
    "train[\"queryCutRMStopWord\"] = train[\"queryCut\"].apply(rm_stop_word)\n",
    "# dev[\"queryCutRMStopWord\"] = dev[\"text\"].apply(rm_stop_word)\n",
    "test[\"queryCutRMStopWord\"] = test[\"queryCut\"].apply(rm_stop_word)\n",
    "print(\"去除停用词\")\n",
    "print(type(train[\"queryCutRMStopWord\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Find_embedding_with_windows(embedding_matrix):\n",
    "    '''\n",
    "    函数说明：该函数用于获取在不同滑动窗口下（size=2,3,4）经过卷积池化操作之后拼接而成的词向量\n",
    "    参数说明：\n",
    "        - embedding_matrix：样本中所有词构成的词向量矩阵\n",
    "    return: 返回拼接而成的一维词向量\n",
    "    '''\n",
    "    # 最终的词向量\n",
    "    result_list = []\n",
    "    for window_size in range(2, 5):\n",
    "        max_list, avg_list = [], []\n",
    "        for k1 in range(len(embedding_matrix)):\n",
    "            if int(k1+window_size) > len(embedding_matrix):\n",
    "                break\n",
    "            else:\n",
    "                matrix01 = embedding_matrix[k1:k1+window_size]\n",
    "                max_list.extend([np.max(matrix01)])  # 最大池化层\n",
    "                avg_list.extend([np.mean(matrix01)])  # 均值池化层\n",
    "        # 再将池化层和均值层拼接起来\n",
    "        max_list.extend(avg_list)\n",
    "        # 将窗口为2，3，4的embedding拼接起来\n",
    "        result_list.extend(max_list)\n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "该函数用于获取标签空间的词嵌入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Find_Label_embedding(example_matrix, embedding):\n",
    "    '''\n",
    "    根据论文《Joint embedding of words and labels》获取标签空间的词嵌入\n",
    "    parameters:\n",
    "    -- example_matrix(np.array 2D): denotes the matrix of words embedding\n",
    "    -- embedding(np.array 2D): denotes the embedding of all words in corpus\n",
    "    return: (np.array 1D) the embedding by join label and word\n",
    "    '''\n",
    "    # 首先在预训练的词向量中获取标签的词向量句子,每一行表示一个标签表示\n",
    "    # 每一行表示一个标签的embedding\n",
    "    label_arr = np.array(\n",
    "        [embedding.wv.get_vector(labelIndexToName[key])\n",
    "         for key in labelIndexToName if labelIndexToName[key] in embedding.wv.vocab.keys()])\n",
    "\n",
    "    # 根据consin来计算label与word之间的相似度,matrix01表示分子\n",
    "    matrix01 = np.dot(label_arr, np.transpose(example_matrix))\n",
    "\n",
    "    # 在计算consin的分母\n",
    "    matrix02 = []\n",
    "    for k1 in range(len(label_arr)):\n",
    "        list01 = []\n",
    "        for k2 in range(len(example_matrix)):\n",
    "            list01.extend([np.linalg.norm(label_arr[k1]) *\n",
    "                           np.linalg.norm(example_matrix[k2])])\n",
    "        matrix02.append(list01)\n",
    "    matrix02 = np.array(matrix02)\n",
    "    # similarity表示通过consin相似度计算得到的矩阵\n",
    "    similarity_matrix = matrix01/matrix02\n",
    "\n",
    "    # 然后对相似矩阵进行均值池化，则得到了“类别-词语”的注意力机制\n",
    "    # 这里可以使用max-pooling和mean-pooling\n",
    "    attention = np.max(similarity_matrix, axis=0)\n",
    "    attention_softmax = softmax(x=attention)\n",
    "    # 将样本的词嵌入与注意力机制相乘得到\n",
    "    attention_embedding = example_matrix * \\\n",
    "        attention_softmax.reshape(len(attention_softmax), 1)\n",
    "    attention_embedding_avg = np.mean(attention_embedding, axis=0)\n",
    "    attention_embedding_max = np.max(attention_embedding, axis=0)\n",
    "    result_embedding = np.hstack(\n",
    "        (attention_embedding_avg, attention_embedding_max))\n",
    "    #print(\"label-word\", result_embedding.shape)\n",
    "\n",
    "    return result_embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "联合多种特征工程来构造新的样本表示，\n",
    "第一： 利用word-embedding的average pooling和max-pooling。\n",
    "第二：分别利用窗口size=2，3，4对word-embedding进行卷积操作，然后再进行max/avg-pooling操作。\n",
    "第三：利用类别标签的表示，增加了词语和标签之间的语义交互，以此达到对词级别语义信息更深层次的考虑。（Label-Embedding Attentive Model (LEAM)）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence2vec(query):\n",
    "    '''\n",
    "    函数说明：联合多种特征工程来构造新的样本表示，主要通过以下三种特征工程方法\n",
    "            第一：利用word-embedding的average pooling和max-pooling\n",
    "            第二：利用窗口size=2，3，4对word-embedding进行卷积操作，然后再进行max/avg-pooling操作\n",
    "            第二：利用类别标签的表示，增加了词语和标签之间的语义交互，以此达到对词级别语义信息更深层次的考虑\n",
    "            另外，对于词向量超过预定义的长度则进行截断，小于则进行填充\n",
    "    参数说明：\n",
    "    - query:数据集中的每一个样本\n",
    "    return: 返回样本经过哦特征工程之后得到的词向量\n",
    "    '''\n",
    "    global max_length\n",
    "    arr = []\n",
    "    # 加载fast_embedding,w2v_embedding\n",
    "    global fast_embedding, w2v_embedding\n",
    "    fast_arr = np.array([fast_embedding.wv.get_vector(s)\n",
    "                         for s in query if s in fast_embedding.wv.vocab.keys()])\n",
    "    # 在fast_arr下滑动获取到的词向量\n",
    "    if len(fast_arr) > 0:\n",
    "        windows_fastarr = np.array(Find_embedding_with_windows(fast_arr))\n",
    "        result_attention_embedding = Find_Label_embedding(\n",
    "            fast_arr, fast_embedding)\n",
    "    else:# 如果样本中的词都不在字典，则该词向量初始化为0\n",
    "        # 这里300表示训练词嵌入设置的维度为300\n",
    "        windows_fastarr = np.zeros(300) \n",
    "        result_attention_embedding = np.zeros(300)\n",
    "\n",
    "    fast_arr_max = np.max(np.array(fast_arr), axis=0) if len(\n",
    "        fast_arr) > 0 else np.zeros(300)\n",
    "    fast_arr_avg = np.mean(np.array(fast_arr), axis=0) if len(\n",
    "        fast_arr) > 0 else np.zeros(300)\n",
    "\n",
    "    fast_arr = np.hstack((fast_arr_avg, fast_arr_max))\n",
    "    # 将多个embedding进行横向拼接\n",
    "    arr = np.hstack((np.hstack((fast_arr, windows_fastarr)),\n",
    "                     result_attention_embedding))\n",
    "    global sentence_max_length\n",
    "    # 如果样本的维度大于指定的长度则需要进行截取或者拼凑,\n",
    "    result_arr = arr[:sentence_max_length] if len(arr) > sentence_max_length else np.hstack((\n",
    "        arr, np.zeros(int(sentence_max_length-len(arr)))))\n",
    "    return result_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "特征选择/抽取函数，由于经过特征工程得到的样本表示维度很高，因此需要进行降维 max_length表示降维之后的样本最大的维度。这里通过PCA方法降维"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Dimension_Reduction(Train, Test):\n",
    "    '''\n",
    "    函数说明：该函数通过PCA算法对样本进行降维，由于目前维度不是特别搞高 ，可以选择不降维。\n",
    "    参数说明：\n",
    "    - Train: 表示训练数据集\n",
    "    - Test: 表示测试数据集\n",
    "    Return: 返回降维之后的数据样本\n",
    "    '''\n",
    "    global max_length\n",
    "    pca = PCA(n_components=max_length)\n",
    "    pca_train = pca.fit_transform(Train)\n",
    "    pca_test = pca.fit_transform(Test)\n",
    "\n",
    "    return pca_train, pca_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成训练集与测试集的词向量并进行归一化处理，获取样本经过特征工程之后的样本表示，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Find_Embedding():\n",
    "    '''\n",
    "    函数说明：该函数用于获取经过特征工程之后的样本表示\n",
    "    Return:训练集特征数组(2D)，测试集特征数组(2D)，训练集标签数组（1D）,测试集标签数组（1D）\n",
    "    '''\n",
    "    print(\"获取样本表示中...\")\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    Train_features2 = min_max_scaler.fit_transform(\n",
    "        np.vstack(train[\"queryCutRMStopWord\"].apply(sentence2vec)))\n",
    "    Test_features2 = min_max_scaler.fit_transform(\n",
    "        np.vstack(test[\"queryCutRMStopWord\"].apply(sentence2vec)))\n",
    "    print(\"获取样本词表示完成\")\n",
    "    # 通过PCA对样本表示进行降维\n",
    "    Train_features2, Test_features2 = Dimension_Reduction(\n",
    "        Train=Train_features2, Test=Test_features2)\n",
    "    Train_label2 = train[\"labelIndex\"]\n",
    "    Test_label2 = test[\"labelIndex\"]\n",
    "\n",
    "    print(\"加载训练好的词向量\")\n",
    "    print(\"Train_features.shape =\", Train_features2.shape)\n",
    "    print(\"Test_features.shape =\", Test_features2.shape)\n",
    "    print(\"Train_label.shape =\", Train_label2.shape)\n",
    "    print(\"Test_label.shape =\", Test_label2.shape)\n",
    "\n",
    "    return Train_features2, Test_features2, Train_label2, Test_label2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过该函数输出模型在训练集和测试集上的准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Predict(Train_label, Test_label, Train_predict_label, Test_predict_label, model_name):\n",
    "    '''\n",
    "    函数说明：直接输出训练集和测试在模型上的准确率\n",
    "    参数说明：\n",
    "        - Train_label: 真实的训练集标签（1D）\n",
    "        - Test_labelb: 真实的测试集标签（1D）\n",
    "        - Train_predict_label: 模型在训练集上的预测的标签(1D)\n",
    "        - Test_predict_label: 模型在测试集上的预测标签（1D）\n",
    "        - model_name: 表示训练好的模型\n",
    "    Return: None\n",
    "    '''\n",
    "    # 输出训练集的准确率\n",
    "    print(Search_Flag+model_name+'_'+'Train accuracy %s' % metrics.accuracy_score(\n",
    "        Train_label, Train_predict_label))\n",
    "    # 输出测试集的准确率\n",
    "    print(Search_Flag+model_name+'_'+'test accuracy %s' % metrics.accuracy_score(\n",
    "        Test_label, Test_predict_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据Grid搜索方法来求模型最优的分类结果参数并保存训练好的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Grid_Train_model(Train_features, Test_features, Train_label, Test_label):\n",
    "    '''\n",
    "    函数说明：基于网格搜索优化的方法搜索模型最优参数，最后保存训练好的模型\n",
    "    参数说明：\n",
    "        - Train_features: 训练集特征数组（2D）\n",
    "        - Test_features: 测试集特征数组（2D）\n",
    "        - Train_label: 真实的训练集标签 (1D)\n",
    "        - Test_label: 真实的测试集标签（1D）\n",
    "    Return: None\n",
    "    '''\n",
    "    parameters = {\n",
    "        'max_depth': [5, 10, 15, 20, 25],\n",
    "        'learning_rate': [0.01, 0.02, 0.05, 0.1, 0.15],\n",
    "        'n_estimators': [100, 500, 1000, 1500, 2000],\n",
    "        'min_child_weight': [0, 2, 5, 10, 20],\n",
    "        'max_delta_step': [0, 0.2, 0.6, 1, 2],\n",
    "        'subsample': [0.6, 0.7, 0.8, 0.85, 0.95],\n",
    "        'colsample_bytree': [0.5, 0.6, 0.7, 0.8, 0.9],\n",
    "        'reg_alpha': [0, 0.25, 0.5, 0.75, 1],\n",
    "        'reg_lambda': [0.2, 0.4, 0.6, 0.8, 1],\n",
    "        'scale_pos_weight': [0.2, 0.4, 0.6, 0.8, 1]\n",
    "\n",
    "    }\n",
    "    # 定义分类模型列表，这里仅使用LightGBM模型\n",
    "    models = [\n",
    "        lgb.LGBMClassifier(objective='multiclass', n_jobs=10, num_class=33, num_leaves=30, reg_alpha=10, reg_lambda=200,\n",
    "                           max_depth=3, learning_rate=0.05, n_estimators=2000, bagging_freq=1, bagging_fraction=0.9, feature_fraction=0.8, seed=1440),\n",
    "    ]\n",
    "    # 遍历模型\n",
    "    for model in models:\n",
    "        model_name = model.__class__.  __name__\n",
    "        gsearch = GridSearchCV(\n",
    "            model, param_grid=parameters, scoring='accuracy', cv=3)\n",
    "        gsearch.fit(Train_features, Train_label)\n",
    "        # 输出最好的参数\n",
    "        print(\"Best parameters set found on development set:{}\".format(\n",
    "            gsearch.best_params_))\n",
    "        Test_predict_label = gsearch.predict(Test_features)\n",
    "        Train_predict_label = gsearch.predict(Train_features)\n",
    "        Predict(Train_label, Test_label,\n",
    "                Train_predict_label, Test_predict_label, model_name)\n",
    "    # 保存训练好的模型\n",
    "    joblib.dump(model, Search_Flag+'_'+model_name+'.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "主函数,先求训练集和测试集的词向量，然后根据Grid搜索来找到最佳参数的分类模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_features, Test_features, Train_label, Test_label = Find_Embedding()\n",
    "Grid_Train_model(Train_features=Train_features, Test_features=Test_features,Train_label=Train_label, Test_label=Test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
